# 成本陷阱
## 数据上线容易下线难
不确定数据表有哪些任务再引用，有哪些人在查询，数据开发不敢停止这个表的数据加工，造成的后果就是数据上线容易下线难
## 低价值的数据应用消耗了大量的资源
数据部门比较关注新的数据产品带给业务的价值，却忽略了已存在的产品或者报表是否还存在价值，最终导致低价值的应用在大量消耗资源
## 烟囱式的开发模式
烟囱式的开发不仅会带来研发效率低的问题，同时因为数据重复加工，还会存在资源浪费的问题
## 数据倾斜
对于spark计算引擎来说，它将海量数据切分成不同的分片(Partition)，分配到不同机器运行的任务中进行并行计算，从而实现计算能力水平扩展。但是整个任务的运行失常，取决于运行最长的那个任务。因为每个分片的数据量可能不同，每个任务需要的资源也不同，由于不同的任务不能分配不同的资源，所以总任务消耗资源=max{单个任务消耗的资源}*任务数量。这样一来数据量小的任务会消耗更多的资源，就会造成数据浪费。
## 数据未设置生命周期

## 调度周期不合理
## 任务参数配置
## 数据未压缩